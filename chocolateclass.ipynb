{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.layouts import column, gridplot\n",
    "from bokeh.transform import dodge\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import methods\n",
    "from collections import defaultdict\n",
    "import importlib as il\n",
    "# from torchsampler import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il.reload(methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './_results'  \n",
    "\n",
    "# Create a dataset and dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 25)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "score_classification = [0, 150, 300, 470, 630] \n",
    "\n",
    "dataset = methods.CustomImageDataset(root_dir=root_dir, transform=transform, classification=score_classification)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "dataset.print_labels(num_samples=5)\n",
    "dc=methods.Container()\n",
    "methods.plot_histogram_and_scatter(dataset, dc=dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Split dataset into train and validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, sampler=methods.CustomImbalancedDatasetSampler(train_dataset), batch_size=32)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, num_epochs, loss function, and optimizer\n",
    "model = methods.ChocolateCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "#training loop\n",
    "avg_val_loss, val_losses, val_accuracy, train_losses, cm, misclassified_images= methods.training_loop(model, num_epochs, criterion, optimizer, train_loader, val_loader, device)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_function = 'CrossEntropyLoss'\n",
    "additional_text = 'Anything'\n",
    "model_class_name = 'ChocolateCNN'\n",
    "score_classification = score_classification \n",
    "\n",
    "# Get the validation indices\n",
    "val_indices = val_dataset.indices if isinstance(val_dataset, torch.utils.data.Subset) else list(range(len(val_dataset)))\n",
    "\n",
    "save_path = 'models/model_with_metadata_sampled2.pth'\n",
    "\n",
    "methods.save_model_with_metadata(\n",
    "    model, optimizer, num_epochs, loss_function, avg_val_loss, val_accuracy,\n",
    "    model_class_name, score_classification, val_losses, train_losses, \n",
    "    additional_text, val_indices, misclassified_images, cm, save_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il.reload(methods)\n",
    "# saved model paths\n",
    "model_paths = [\n",
    "    # 'models/chocolate_cnn1.pth',\n",
    "    # 'models/chocolate_cnn_best.pth',\n",
    "    # 'models/chocolate_cnn.pth',\n",
    "    'models/model_with_metadata_sampled3.pth',\n",
    "    'models/model_with_metadata_sampled4.pth'\n",
    "]\n",
    "\n",
    "# Dictionary to store misclassified filenames for each model\n",
    "misclassified_details = defaultdict(dict)\n",
    "\n",
    "for model_path in model_paths:\n",
    "    print(f\"Evaluating model: {model_path}\")\n",
    "\n",
    "    (model, optimizer, num_epochs, criterion, additional_text, valid_loss, \n",
    "    valid_accuracy, score_classification, misclassified_images, cm, val_indices,train_losses, val_losses) = methods.load_model_with_metadata(model_path,device)\n",
    "    print()\n",
    "    # Reconstruct the validation DataLoader\n",
    "    loaded_dataset = methods.CustomImageDataset(root_dir='./_results', transform=transform, classification=score_classification)\n",
    "    val_dataset = torch.utils.data.Subset(loaded_dataset, val_indices)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Store misclassified details\n",
    "    for info in misclassified_images:\n",
    "        if info['filename'] not in misclassified_details[model_path]:\n",
    "            misclassified_details[model_path][info['filename']] = {'image': info['image'], 'details': []}\n",
    "        misclassified_details[model_path][info['filename']]['details'].append({\n",
    "            'true': info['label'],\n",
    "            'pred': info['predicted'],\n",
    "            'score': info['score']\n",
    "        })\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    methods.plot_losses(num_epochs, train_losses, val_losses, title=f'Training and Validation Loss for {model_path}')\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    methods.plot_confusion_matrix(cm, class_names=[0, 1, 2, 3, 4])  # Adjust class names as needed\n",
    "\n",
    "    # Show misclassified images\n",
    "    methods.show_misclassified_images(misclassified_images)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common misclassified images across all models\n",
    "common_misclassified_images = set(misclassified_details[model_paths[0]].keys())\n",
    "for model_path in model_paths[1:]:\n",
    "    common_misclassified_images.intersection_update(misclassified_details[model_path].keys())\n",
    "\n",
    "print(f\"Common misclassified images: {len(common_misclassified_images)}\")\n",
    "\n",
    "\n",
    "# Show common misclassified images for the last model loaded (or any of the models)\n",
    "methods.show_common_misclassified_images(common_misclassified_images, misclassified_details, model_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('univenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ccd40bcc000b7a7cf40402755891779d0f67280314eb8adf598b00e0d55ca52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
