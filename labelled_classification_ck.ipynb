{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ck/miniconda3/envs/chimcla311/bin/python Mon May 19 09:34:25 2025\n"
     ]
    }
   ],
   "source": [
    "import ipydex, sys, time, os\n",
    "print(sys.executable, time.ctime())\n",
    "%load_ext ipydex.displaytools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib as il\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib as il\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "\n",
    "import classification_methods as clm\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install grad-cam\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups read: 15\n"
     ]
    }
   ],
   "source": [
    "from approach2 import model_evaluation as me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups read: 15\n"
     ]
    }
   ],
   "source": [
    "il.reload(clm)\n",
    "il.reload(me)\n",
    "\n",
    "m = me.Manager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/hdd-a2/space/Nextcloud/IEE-GE/XAI-DIA/ChocolateClassification'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "me.PARENT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChocolateCNN\n",
      "ChocolateCNN\n",
      "Loaded model type: ChocolateCNN, epoch: 20, validation accuracy: 97.37076648841355%\n"
     ]
    }
   ],
   "source": [
    "m.load_model(f\"{me.PARENT_DIR}/classification_models/CNN_3.pth\")\n",
    "# m.plot_training_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "150it [00:00, 559.51it/s]\n"
     ]
    }
   ],
   "source": [
    "m.make_predictions(limit=150, display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChocolateCNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=9600, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=512, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_str(img_tensor) -> str:\n",
    "    with torch.no_grad():\n",
    "        image_outputs = m.model(img_tensor)\n",
    "        predicted_probs = torch.sigmoid(image_outputs)  # Apply sigmoid for multi-label classification\n",
    "        predicted_num_labels = (predicted_probs > 0.5).int()  # Convert probabilities to binary predictions\n",
    "\n",
    "    predicted_num_labels = [int(elt) for elt in predicted_num_labels.cpu().numpy().flatten()]\n",
    "\n",
    "    res = []\n",
    "    # print(predicted_probs)\n",
    "    # print(predicted_num_labels)\n",
    "    for ln, num_label, prob in zip(\n",
    "        me.label_names, predicted_num_labels, predicted_probs.cpu().numpy().flatten()\n",
    "    ):\n",
    "        if num_label:\n",
    "            res.append(f\"{ln} ({round(float(prob), 3)})\")\n",
    "\n",
    "    res_str = \"\\n\".join(res)\n",
    "    return res_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fpath = \"\" # m.img_fpaths_for_prediction[4]\n",
    "plt.rcParams['figure.subplot.top'] = .8\n",
    "\n",
    "def grad_cam_cell(img_fpath):\n",
    "    # usually the last convolutional layer\n",
    "    cam = GradCAM(model=m.model, target_layers=[m.model.conv2])\n",
    "    # Prepare the target\n",
    "    # targets = [ClassifierOutputTarget(class_idx)]\n",
    "    # q = m.load_and_preprocess_image(img_fpath)\n",
    "\n",
    "    img_tensor = m.load_and_preprocess_image(img_fpath)\n",
    "    label_str = get_label_str(img_tensor)\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=img_tensor, targets=None)\n",
    "\n",
    "    cmap_cam = plt.cm.viridis(grayscale_cam)[0, :, : , :3]\n",
    "\n",
    "    image_np = m.np_img_from_tensor(img_tensor)\n",
    "\n",
    "    # note  cv2.resize takes x dimension (col index) first\n",
    "    # image1 = me.cv2.resize(image, (25, 100)) ##:i\n",
    "\n",
    "    # Overlay the heatmap on the original image\n",
    "    # cam_image = show_cam_on_image(image_np, grayscale_cam[0], use_rgb=True)\n",
    "\n",
    "    full_image = np.concatenate((image_np, cmap_cam), axis=1)\n",
    "    res_fpath = img_fpath.replace(\"predict_inputs\", \"gradcam_results2\")\n",
    "    dirpath, _ = os.path.split(res_fpath)\n",
    "    os.makedirs(dirpath, exist_ok=True)\n",
    "    plt.title(label_str)\n",
    "    plt.imshow(full_image)\n",
    "    plt.savefig(res_fpath, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:19<00:00,  7.55it/s]\n"
     ]
    }
   ],
   "source": [
    "for img_fpath in me.tqdm.tqdm(m.img_fpaths_for_prediction[:]):\n",
    "    grad_cam_cell(img_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 55,  42, 132],\n",
       "        [ 54,  42, 132],\n",
       "        [ 51,  38, 126],\n",
       "        ...,\n",
       "        [ 53,  38, 128],\n",
       "        [ 55,  41, 132],\n",
       "        [ 79,  65, 156]],\n",
       "\n",
       "       [[ 57,  44, 134],\n",
       "        [ 53,  40, 131],\n",
       "        [ 54,  40, 129],\n",
       "        ...,\n",
       "        [ 58,  43, 132],\n",
       "        [ 54,  40, 131],\n",
       "        [ 64,  50, 141]],\n",
       "\n",
       "       [[ 55,  41, 132],\n",
       "        [ 55,  41, 131],\n",
       "        [ 51,  38, 127],\n",
       "        ...,\n",
       "        [ 54,  39, 128],\n",
       "        [ 56,  41, 132],\n",
       "        [ 65,  50, 141]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 55,  38, 129],\n",
       "        [ 59, 150, 227],\n",
       "        [251,  59,  34],\n",
       "        ...,\n",
       "        [ 60, 144, 226],\n",
       "        [249, 199,  34],\n",
       "        [243,  51,  30]],\n",
       "\n",
       "       [[ 55,  40, 129],\n",
       "        [ 54,  39, 194],\n",
       "        [ 53, 158, 223],\n",
       "        ...,\n",
       "        [ 56,  41, 196],\n",
       "        [ 54, 112, 224],\n",
       "        [ 55, 161, 225]],\n",
       "\n",
       "       [[ 63,  48, 136],\n",
       "        [ 58,  45, 132],\n",
       "        [ 55,  40, 131],\n",
       "        ...,\n",
       "        [ 62,  48, 137],\n",
       "        [ 64,  52, 140],\n",
       "        [ 74,  61, 150]]], dtype=uint8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((image1, cam_image), axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayscale_cam[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 24, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mGradCAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseCAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mreshape_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mGradCAM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtarget_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mreshape_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mget_cam_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0mtarget_layer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0mtarget_category\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                        \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# 2D image\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# 3D image\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid grads shape.\"\u001b[0m \u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                             \u001b[0;34m\"Shape of grads should be 4 (2D image) or 5 (3D image).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           /media/sd-card-disk/miniconda3/envs/chimcla311/lib/python3.11/site-packages/pytorch_grad_cam/grad_cam.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "GradCAM??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chimcla311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
